{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                            Machine Learning with Imbalanced Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMBALANCED DATASET - The dataset may contain uneven samples /instances , so that it makes the algorithm to predict with accuracy of 1.0 each time u run the model. For example, if u have simple dataset with 4 features and output(target) feature with 2 class, then total no. of instances/samples be 100. Now, out of 100, 80 instances belongs to category1 of the output(target) feature and only 20 instances contribute to the category2 of the output(target) feature. So, obviously, this makes bias in training and predicting the model. So, this dataset refers to Imbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Neccessary Packages and reading the csv file and printing the head of the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Mcg   Gvh   Lip  Chg   Aac  Alm1  Alm2     Class\n",
      "0  0.49  0.29  0.48  0.5  0.56  0.24  0.35  positive\n",
      "1  0.07  0.40  0.48  0.5  0.54  0.35  0.44  positive\n",
      "2  0.56  0.40  0.48  0.5  0.49  0.37  0.46  positive\n",
      "3  0.59  0.49  0.48  0.5  0.52  0.45  0.36  positive\n",
      "4  0.23  0.32  0.48  0.5  0.55  0.25  0.35  positive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "file = pd.read_csv(\"ecoli.csv\",sep=',')\n",
    "print(file.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Checking whether any column in the dataset contains NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the Basic Statistics(Descriptive) of the \"Class\" feature in the dataset. It shows that there are two unique values(positive and negative), with positive value counts upto 143 and negative 77."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count          220\n",
       "unique           2\n",
       "top       positive\n",
       "freq           143\n",
       "Name: Class, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file['Class'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we just grouped the datset based on the 'class' feature to visualize the counts of positive and negative values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mcg</th>\n",
       "      <th>Gvh</th>\n",
       "      <th>Lip</th>\n",
       "      <th>Chg</th>\n",
       "      <th>Aac</th>\n",
       "      <th>Alm1</th>\n",
       "      <th>Alm2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>143</td>\n",
       "      <td>143</td>\n",
       "      <td>143</td>\n",
       "      <td>143</td>\n",
       "      <td>143</td>\n",
       "      <td>143</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Mcg  Gvh  Lip  Chg  Aac  Alm1  Alm2\n",
       "Class                                        \n",
       "negative   77   77   77   77   77    77    77\n",
       "positive  143  143  143  143  143   143   143"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = file.groupby(\"Class\")\n",
    "f.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are converting the 'class' feature from text to int using .map function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: Class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "file['Class'] = file['Class'].map({'positive': 1, 'negative': 0})\n",
    "print(file['Class'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the sklearn library, we import train_test_test from cross validation and split the original dataset into training and test dataset(80,20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(176, 8)\n",
      "(44, 8)\n",
      "(176, 7)\n",
      "(44, 7)\n",
      "(176,)\n",
      "(44,)\n",
      "99     1\n",
      "137    1\n",
      "65     1\n",
      "124    1\n",
      "12     1\n",
      "Name: Class, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train, test = train_test_split(file,test_size=0.2)\n",
    "features_train=train[['Mcg','Gvh','Lip','Chg','Aac','Alm1','Alm2']]\n",
    "features_test = test[['Mcg','Gvh','Lip','Chg','Aac','Alm1','Alm2']]\n",
    "labels_train = train.Class\n",
    "labels_test = test.Class\n",
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(features_train.shape)\n",
    "print(features_test.shape)\n",
    "print(labels_train.shape)\n",
    "print(labels_test.shape)\n",
    "print(labels_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the accuracy of the noraml model using various classifiers, but it clearly shows that most of the ML algorithm predicts with the accuracy of 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.954545454545\n",
      "Accuracy: 0.977272727273\n",
      "Accuracy: 0.954545454545\n",
      "Accuracy: 0.886363636364\n",
      "Accuracy: 1.0\n",
      "Accuracy: 0.977272727273\n",
      "Accuracy: 1.0\n",
      "Accuracy: 1.0\n",
      "Accuracy: 1.0\n",
      "Accuracy: 1.0\n",
      "Accuracy: 0.977272727273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "names = ['GaussianNB','SVM Linear','SVM Rbf','SVM Poly','SVM Linear C','SVM Rbf C','SVM Poly C','Decision tree','Decision tree with minsample','k neighbors','k neighbors with n']\n",
    "classifiers = [GaussianNB(),\n",
    "               SVC(kernel=\"linear\"),\n",
    "               SVC(kernel=\"rbf\"),\n",
    "               SVC(kernel=\"poly\"),\n",
    "               SVC(kernel=\"linear\", C=1000),\n",
    "               SVC(kernel=\"rbf\", C=1000),\n",
    "               SVC(kernel=\"poly\", C=1000),\n",
    "               DecisionTreeClassifier(),\n",
    "               DecisionTreeClassifier(min_samples_split=5),\n",
    "               KNeighborsClassifier(),\n",
    "               KNeighborsClassifier(n_neighbors=2),\n",
    "               RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "               AdaBoostClassifier()]\n",
    "for name,clf in zip(names,classifiers):\n",
    "    clf.fit(features_train,labels_train)\n",
    "    print(\"Accuracy:\",clf.score(features_test,labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There two main ways to handle the Imbalanced datset:\n",
    "\n",
    "1.Over Sampling\n",
    "2.Under Sampling\n",
    "\n",
    "OVER SAMPLING:\n",
    "It is nothing but Sampling the minority class and making it equivalent to the majority class\n",
    "Ex:before sampling: Counter({1: 111, 0: 65})\n",
    "after sampling: Counter({1: 111, 0: 111})\n",
    "Note:The counts of 1's and 0's before and after sampling\n",
    "\n",
    "\n",
    "UNDER SAMPLING:\n",
    "It is nothing but Sampling the majority class and making it equivalent to the minority class\n",
    "Ex:before sampling: Counter({1: 111, 0: 65})\n",
    "after sampling: Counter({0: 65, 1: 65})\n",
    "\n",
    "\n",
    "There are several algorithms for over sampling and under sampling. The one we use here is,\n",
    "\n",
    "OVER SAMPLING ALGORITHM:\n",
    "1.SMOTE - Synthetic Minority Over Sampling Technique\n",
    "A subset of data is taken from the minority class as an example and then new synthetic similar instances are created. These synthetic instances are then added to the original dataset. The new dataset is used as a sample to train the classification models\n",
    "\n",
    "UNDER SAMPLING ALGORITHM:\n",
    "1.RandomUnderSampler - Random Undersampling aims to balance class distribution by randomly eliminating majority class examples.  This is done until the majority and minority class instances are balanced out.\n",
    "\n",
    "2.NearMiss - selects the majority class samples whose average distances to three closest minority class samples are the smallest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before sampling: Counter({1: 111, 0: 65})\n",
      "after sampling: Counter({1: 111, 0: 111})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "X_resampled, y_resampled = SMOTE(kind='borderline1').fit_sample(features_train, labels_train)\n",
    "print(\"before sampling:\",format(Counter(labels_train)))\n",
    "print(\"after sampling:\",format(Counter(y_resampled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.977272727273\n",
      "Accuracy: 0.977272727273\n",
      "Accuracy: 0.954545454545\n",
      "Accuracy: 0.954545454545\n",
      "Accuracy: 0.977272727273\n",
      "Accuracy: 1.0\n",
      "Accuracy: 1.0\n",
      "Accuracy: 1.0\n",
      "Accuracy: 1.0\n",
      "Accuracy: 1.0\n",
      "Accuracy: 0.977272727273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
    "names = ['GaussianNB','SVM Linear','SVM Rbf','SVM Poly','SVM Linear C','SVM Rbf C','SVM Poly C','Decision tree','Decision tree with minsample','k neighbors','k neighbors with n']\n",
    "classifiers = [GaussianNB(),\n",
    "               SVC(kernel=\"linear\"),\n",
    "               SVC(kernel=\"rbf\"),\n",
    "               SVC(kernel=\"poly\"),\n",
    "               SVC(kernel=\"linear\", C=1000),\n",
    "               SVC(kernel=\"rbf\", C=1000),\n",
    "               SVC(kernel=\"poly\", C=1000),\n",
    "               DecisionTreeClassifier(),\n",
    "               DecisionTreeClassifier(min_samples_split=5),\n",
    "               KNeighborsClassifier(),\n",
    "               KNeighborsClassifier(n_neighbors=2),\n",
    "               RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "               AdaBoostClassifier()]\n",
    "for name,clf in zip(names,classifiers):\n",
    "    clf.fit(X_resampled, y_resampled)\n",
    "    print(\"Accuracy:\",clf.score(features_test,labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before sampling: Counter({1: 111, 0: 65})\n",
      "after sampling: Counter({0: 65, 1: 65})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "X_resampled, y_resampled = rus.fit_sample(features_train, labels_train)\n",
    "print(\"before sampling:\",format(Counter(labels_train)))\n",
    "print(\"after sampling:\",format(Counter(y_resampled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.977272727273\n",
      "Accuracy: 0.977272727273\n",
      "Accuracy: 0.977272727273\n",
      "Accuracy: 0.954545454545\n",
      "Accuracy: 1.0\n",
      "Accuracy: 1.0\n",
      "Accuracy: 1.0\n",
      "Accuracy: 0.931818181818\n",
      "Accuracy: 0.954545454545\n",
      "Accuracy: 1.0\n",
      "Accuracy: 0.931818181818\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "names = ['GaussianNB','SVM Linear','SVM Rbf','SVM Poly','SVM Linear C','SVM Rbf C','SVM Poly C','Decision tree','Decision tree with minsample','k neighbors','k neighbors with n']\n",
    "classifiers = [GaussianNB(),\n",
    "               SVC(kernel=\"linear\"),\n",
    "               SVC(kernel=\"rbf\"),\n",
    "               SVC(kernel=\"poly\"),\n",
    "               SVC(kernel=\"linear\", C=1000),\n",
    "               SVC(kernel=\"rbf\", C=1000),\n",
    "               SVC(kernel=\"poly\", C=1000),\n",
    "               DecisionTreeClassifier(),\n",
    "               DecisionTreeClassifier(min_samples_split=5),\n",
    "               KNeighborsClassifier(),\n",
    "               KNeighborsClassifier(n_neighbors=2),\n",
    "               RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "               AdaBoostClassifier()]\n",
    "for name,clf in zip(names,classifiers):\n",
    "    clf.fit(X_resampled, y_resampled)\n",
    "    print(\"Accuracy:\",clf.score(features_test,labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before sampling: Counter({1: 111, 0: 65})\n",
      "after sampling: Counter({0: 65, 1: 65})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from imblearn.under_sampling import NearMiss\n",
    "rus = NearMiss(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_sample(features_train, labels_train)\n",
    "print(\"before sampling:\",format(Counter(labels_train)))\n",
    "print(\"after sampling:\",format(Counter(y_resampled)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.977272727273\n",
      "Accuracy: 0.977272727273\n",
      "Accuracy: 0.954545454545\n",
      "Accuracy: 0.886363636364\n",
      "Accuracy: 1.0\n",
      "Accuracy: 0.977272727273\n",
      "Accuracy: 1.0\n",
      "Accuracy: 0.977272727273\n",
      "Accuracy: 0.977272727273\n",
      "Accuracy: 1.0\n",
      "Accuracy: 0.977272727273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "names = ['GaussianNB','SVM Linear','SVM Rbf','SVM Poly','SVM Linear C','SVM Rbf C','SVM Poly C','Decision tree','Decision tree with minsample','k neighbors','k neighbors with n']\n",
    "classifiers = [GaussianNB(),\n",
    "               SVC(kernel=\"linear\"),\n",
    "               SVC(kernel=\"rbf\"),\n",
    "               SVC(kernel=\"poly\"),\n",
    "               SVC(kernel=\"linear\", C=1000),\n",
    "               SVC(kernel=\"rbf\", C=1000),\n",
    "               SVC(kernel=\"poly\", C=1000),\n",
    "               DecisionTreeClassifier(),\n",
    "               DecisionTreeClassifier(min_samples_split=5),\n",
    "               KNeighborsClassifier(),\n",
    "               KNeighborsClassifier(n_neighbors=2),\n",
    "               RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "               AdaBoostClassifier()]\n",
    "for name,clf in zip(names,classifiers):\n",
    "    clf.fit(X_resampled, y_resampled)\n",
    "    print(\"Accuracy:\",clf.score(features_test,labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
